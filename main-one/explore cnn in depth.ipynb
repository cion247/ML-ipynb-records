{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdVS6of3i8ucJJlEIpvLgn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n"],"metadata":{"id":"Ah3Il-z-vc1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdnZ5ev7tKg8"},"outputs":[],"source":["\n","# Define the model\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 256)  # Adjusted input size after pooling\n","        self.fc2 = nn.Linear(256, 10)  # Output will be 10 classes for CIFAR-10\n","\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=0.5)  # Adding dropout for regularization\n","\n","    def forward(self, x):\n","        x = self.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = self.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.relu(self.conv3(x))\n","        x = self.pool(x)\n","        x = x.view(-1, 64 * 4 * 4)  # Adjusted size after pooling\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n"]},{"cell_type":"code","source":["# Load CIFAR-10 dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UCqgCqltPzk","executionInfo":{"status":"ok","timestamp":1709308835885,"user_tz":-330,"elapsed":7561,"user":{"displayName":"Xocion","userId":"15389155811112520621"}},"outputId":"e3447fa3-a7d8-40d8-debc-3bf3ce8d144e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 60543647.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import StepLR\n","\n","# Define the model, loss function, and optimizer\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = SimpleCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"],"metadata":{"id":"u6VBcZ-yv2Md"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","loss_values = []\n","\n","for epoch in range(5):  # Adjust number of epochs as needed\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 200 == 199:  # Print every 200 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 200))\n","            loss_values.append(running_loss / 200)\n","            running_loss = 0.0\n","\n","print('Finished Training')\n","\n","# Plot the loss graph\n","plt.plot(loss_values)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training Loss')\n","plt.show()\n"],"metadata":{"id":"w0UV5mkYwDmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct = 0\n","total = 0\n","\n","# Disable gradient calculation for evaluation\n","model.eval()\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data[0].to(device), data[1].to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy on the test set: %d %%' % (100 * correct / total))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aofoBV1lxMsO","executionInfo":{"status":"ok","timestamp":1709309790914,"user_tz":-330,"elapsed":7853,"user":{"displayName":"Xocion","userId":"15389155811112520621"}},"outputId":"a303c603-2074-46b4-a49d-f3ec6f45cb6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on the test set: 65 %\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","# Define the model\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 256)  # Adjusted input size after pooling\n","        self.fc2 = nn.Linear(256, 10)  # Output will be 10 classes for CIFAR-10\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=0.5)  # Adding dropout for regularization\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        x = self.pool(self.relu(self.conv3(x)))\n","        x = x.view(-1, 64 * 4 * 4)  # Adjusted size after pooling\n","        x = self.dropout(self.relu(self.fc1(x)))\n","        x = self.fc2(x)\n","        return x\n","\n","# Load CIFAR-10 dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n","\n","# Define the model, loss function, and optimizer\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = SimpleCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Training loop\n","num_epochs = 5\n","loss_values = []\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 200 == 199:  # Print every 200 mini-batches\n","            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n","            loss_values.append(running_loss / 200)\n","            running_loss = 0.0\n","\n","print('Finished Training')\n","\n","# Plot the loss graph\n","plt.plot(loss_values)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training Loss')\n","plt.show()\n","\n","# Evaluation\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data[0].to(device), data[1].to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print('Accuracy on the test set: %.2f %%' % accuracy)\n"],"metadata":{"id":"mmQAJ6n7I7-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import time\n","# Record the start time\n","start_time = time.time()\n","\n","# Call your function here\n","\n","def convolve2d(input_matrix, kernel):\n","    # Get the dimensions of the input matrix and kernel\n","    input_height, input_width = input_matrix.shape\n","    kernel_height, kernel_width = kernel.shape\n","\n","    # Compute the output size\n","    output_height = input_height - kernel_height + 1\n","    output_width = input_width - kernel_width + 1\n","\n","    # Initialize the output feature map\n","    output = np.zeros((output_height, output_width))\n","\n","    # Flip the kernel (required for convolution)\n","    kernel = np.flipud(np.fliplr(kernel))\n","\n","    # Perform 2D convolution\n","    for i in range(output_height):\n","        for j in range(output_width):\n","            output[i, j] = np.sum(input_matrix[i:i+kernel_height, j:j+kernel_width] * kernel)\n","\n","    return output\n","\n","# Example usage:\n","input_matrix = np.array([[0,0, 0, 0,0],\n","                         [0,1, 2, 3,0],\n","                         [0,4, 5, 6,0],\n","                         [0,7, 8, 9,0],\n","                         [0,0, 0, 0,0]])\n","\n","kernel = np.array([[1, 0, -1],\n","                   [2, 0, -2],\n","                   [1, 0, -1]])\n","\n","output_feature_map = convolve2d(input_matrix, kernel)\n","print(\"Output Feature Map:\")\n","print(output_feature_map)\n","\n","# Record the end time\n","end_time = time.time()\n","\n","# Calculate the elapsed time\n","elapsed_time = end_time - start_time\n","print(\"Elapsed time:\", elapsed_time, \"seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSM27ixTM-aN","executionInfo":{"status":"ok","timestamp":1709350995758,"user_tz":-330,"elapsed":436,"user":{"displayName":"Xocion","userId":"15389155811112520621"}},"outputId":"832f58ef-0e31-4247-96b5-f5da9c55af82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output Feature Map:\n","[[  9.   6.  -9.]\n"," [ 20.   8. -20.]\n"," [ 21.   6. -21.]]\n","Elapsed time: 0.002889871597290039 seconds\n"]}]},{"cell_type":"code","source":["start_time = time.time()\n","a = np.convolve([1, 2, 3,4, 5, 6,7, 8, 9],[1, 0, -1,2, 0, -2,1, 0, -1])\n","print(a)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(\"Elapsed time:\", elapsed_time, \"seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62N1oyGtNJ_x","executionInfo":{"status":"ok","timestamp":1709350855398,"user_tz":-330,"elapsed":5,"user":{"displayName":"Xocion","userId":"15389155811112520621"}},"outputId":"ef81c6db-2f59-4123-fd07-6481b51d09ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[  1   2   2   4   6   6   7   8   8  -2  -3   6 -14 -16   2  -8  -9]\n","Elapsed time: 0.0007612705230712891 seconds\n"]}]},{"cell_type":"code","source":["# [[[0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0]],\n","#                              [[0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0]],\n","#                              [[0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0]],\n","#                              [[0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0],\n","#                               [0,0,0,0,0,0]]\n","\n","#                             ,[[1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1]]\n","#                               ,[[1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1]]\n","#                               ,[[1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1]]\n","#                               ,[[1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1],\n","#                               [1,1,1,1,1,1]]\n","\n","#                             ,[[2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2]]\n","#                               ,[[2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2]]\n","#                               ,[[2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2]]\n","#                               ,[[2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2],\n","#                               [2,2,2,2,2,2]]\n","#                               ]"],"metadata":{"id":"22UGuDlLGrTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_array = [[[0]],[[0]],[[0]],[[0]],[[1]],[[1]],[[1]],[[1]],[[2]],[[2]],[[2]],[[2]]]\n","input_array = np.array(input_array)\n","print(input_array.shape)\n","input_array[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_8BjfqKwzyD","executionInfo":{"status":"ok","timestamp":1709913179522,"user_tz":-330,"elapsed":438,"user":{"displayName":"Xocion","userId":"15389155811112520621"}},"outputId":"6a54f93c-483e-4731-d81f-9a133b763eac"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["(12, 1, 1)\n"]},{"output_type":"execute_result","data":{"text/plain":["(1, 1)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# Define the input array\n","input_array = [[[0]],[[0]],[[0]],[[0]],[[1]],[[1]],[[1]],[[1]],[[2]],[[2]],[[2]],[[2]]]\n","\n","# Define the group size\n","group_size = 3\n","\n","# Initialize the resulting array\n","result_array = []\n","\n","# Loop through the input array in chunks of group_size\n","for i in range(0, len(input_array), group_size):\n","    # Extract the chunk\n","    chunk = input_array[i:i+group_size]\n","    # Rotate the chunk by one position\n","    rotated_chunk = chunk[1:] + chunk[:1]\n","    # Append the rotated chunk to the resulting array\n","    result_array.extend(rotated_chunk)\n","\n","print(result_array)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPDyVMZ8FDF1","executionInfo":{"status":"ok","timestamp":1709911501993,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Xocion","userId":"15389155811112520621"}},"outputId":"fce2b402-5d86-4f3b-dabf-95cc7d0a7ae0"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0]], [[0]], [[0]], [[1]], [[1]], [[0]], [[1]], [[2]], [[1]], [[2]], [[2]], [[2]]]\n"]}]}]}